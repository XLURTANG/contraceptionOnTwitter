{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the tweets extracted and filtering\n",
    "\n",
    "This notebook contains the different functions developed to:\n",
    "- unify the files of the same category but extracted based on a different keyword\n",
    "- remove unnecesary characters, get rid of URLS, hastaghs, etc. \n",
    "- remove tweets where the username is the keyword\n",
    "- remove tweets with keywords from multiple cateogories\n",
    "\n",
    "When running this notebook, we saved the output at two different step, first, before removing those tweets that contain keywords from multiple categories (semiclean) and the final ones after all the filtering steps (cleanTweets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "✔ ggplot2 3.1.1     ✔ readr   1.3.1\n",
      "✔ tibble  2.1.1     ✔ purrr   0.3.2\n",
      "✔ tidyr   0.8.3     ✔ stringr 1.4.0\n",
      "✔ ggplot2 3.1.1     ✔ forcats 0.4.0\n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Load Libraries    #\n",
    "#####################\n",
    "#install.packages(c(\"tidyverse\",\"dplyr\"))\n",
    "library(\"dplyr\")\n",
    "library(\"tidyverse\")\n",
    "library(\"ggplot2\")\n",
    "rm(list=ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Unify Files       #\n",
    "#####################\n",
    "unifyFiles <- function( folder_path, folder_name ){\n",
    "  \n",
    "  # create a unique file with all the tweets present in that folder\n",
    "  files <- list.files( paste0( folder_path, folder_name ))\n",
    "  \n",
    "  for( i in 1:length( files ) ){\n",
    "    \n",
    "    print( paste0( \"Reading the file \", files[i] ) ) \n",
    "    if( i == 1 ){\n",
    "      tweets <- read.csv(paste0(folder_path, folder_name, files[i]), sep = \",\")\n",
    "      print( paste0( \"For the keyword \", files[i], \" there are a total of \", nrow( tweets)))\n",
    "    }else{\n",
    "      new_tweet <- read.csv( paste0(folder_path, folder_name, files[i]), sep = \",\")\n",
    "      print( paste0( \"For the keyword \", files[i], \" there are a total of \", nrow( new_tweet)))\n",
    "      tweets<- rbind( tweets , new_tweet )\n",
    "    }\n",
    "    \n",
    "  }\n",
    "  return( tweets)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Clean the text in the tweets to prepare for sentiment analysis #\n",
    "##################################################################\n",
    "cleanTweets <- function( tweets.df ){\n",
    "    tweets.df$text <- gsub(\"&amp;\", \"and\", tweets.df$text) # Replace & symbol with \"and\" when coded as &amp;\n",
    "    tweets.df$text <- gsub(\"&gt;\", \"\", tweets.df$text ) # Get rid of  > symbol when coded as &gt;\n",
    "    tweets.df$text <- gsub(\"&lt;\", \"\", tweets.df$text ) # Get rid of  < symbol when coded as &lt;\n",
    "    tweets.df$text <- gsub(\"&#13;\", \" \", tweets.df$text ) # Get rid of  html symbol for Carriage Return &#13;\n",
    "    tweets.df$text <- str_replace_all(tweets.df$text ,\" \",\" \")          #Get rid of unnecessary spaces\n",
    "    tweets.df$text <- gsub(\"“\", \"\", tweets.df$text) # Get rid of quotes;\n",
    "    tweets.df$text <- gsub(\"”\", \"\", tweets.df$text) # Get rid of quotes;\n",
    "    tweets.df$text <- gsub(\"\\\"\", \"\", tweets.df$text) # Get rid of quotes;\n",
    "    tweets.df$text <- gsub(\"—\", \"\", tweets.df$text) # Get rid of quotes;\n",
    "    tweets.df$text <- str_replace_all(tweets.df$text ,\"#[a-z,A-Z]*\",\"\") # Get rid of hashtags\n",
    "    tweets.df$text <- gsub(\"(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)\", \"\",tweets.df$text )# Take out retweet header / via @\n",
    "    tweets.df$text <- str_replace_all(tweets.df$text,\"@[a-z,A-Z]*\",\"\")  # Get rid of references to other screennames\n",
    "    tweets.df$text <- gsub(\"http[[:alnum:][:punct:]]*\", \"\", tweets.df$text ) # Get rid of URLs\n",
    "    tweets.df$text <- gsub(\"www[[:alnum:][:punct:]]*\", \"\", tweets.df$text ) # Get rid of URLs that do not start with http\n",
    "    tweets.df$text <- gsub(\"pic.twitter[[:alnum:][:punct:]]*\", \"\", tweets.df$text ) # Get rid of pic.twitter\n",
    "    tweets.df$text <- trimws( tweets.df$text ) # Get rid of leading and/or trailing whitespace \n",
    "    return( tweets.df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# To remove tweets where the username is the keyword #\n",
    "######################################################\n",
    "usernameClean <- function( tweets.df, category ){\n",
    "  \n",
    "  tweets.df$text <- tolower( tweets.df$text )\n",
    "  tweets.df$username <- tolower( tweets.df$username )\n",
    "  \n",
    "  wclist <- read.delim(\"/home/ec2-user/SageMaker/CategoryKeyword\", sep = \"\\t\", header = TRUE)\n",
    "  wclist$Category <- tolower( wclist$Category )\n",
    "  wclist$Keyword  <- tolower( wclist$Keyword )\n",
    "  \n",
    "  keywords <- trimws( wclist[ wclist$Category == tolower(category), \"Keyword\"] )\n",
    "  \n",
    "  \n",
    "  \n",
    "  if( length(grep(paste(keywords,collapse=\"|\"), \n",
    "           tolower(tweets.df$username)) ) != 0){\n",
    "    tweets.df <- tweets.df[ -c( (grep(paste(keywords,collapse=\"|\"), \n",
    "                                      tweets.df$username))), ]\n",
    "  }\n",
    "\n",
    "  return( tweets.df)\n",
    "  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# To remove tweets with keywords from multiple cateogories #\n",
    "############################################################\n",
    "\n",
    "tweetsUniqueCategory <- function( tweets.df, category){\n",
    "    \n",
    "    wclist <- read.delim(\"/home/ec2-user/SageMaker/CategoryKeyword\", sep = \"\\t\", header = TRUE)\n",
    "    wclist$Category <- tolower( wclist$Category )\n",
    "    wclist$Keyword  <- tolower( wclist$Keyword )\n",
    "    \n",
    "    if( category == \"LNG IUD\"| category == \"Copper IUD\"){\n",
    "          excludedKeywords <- trimws( wclist[ ( wclist$Category != tolower(category) & wclist$Category != tolower(\"IUD\") ), \"Keyword\"] )\n",
    "          includedKeywords <- trimws( wclist[ wclist$Category == tolower(category), \"Keyword\"] )\n",
    "    }else{\n",
    "          excludedKeywords <- trimws( wclist[ wclist$Category != tolower(category), \"Keyword\"] )\n",
    "          includedKeywords <- trimws( wclist[ wclist$Category == tolower(category), \"Keyword\"] )\n",
    "    }\n",
    "    \n",
    "  \n",
    "  excluded.tweets <- grep(paste(excludedKeywords,collapse=\"|\"), \n",
    "                                    tweets.df$text)\n",
    "  included.tweets <- grep(paste(includedKeywords,collapse=\"|\"), \n",
    "                          tweets.df$text)\n",
    "  \n",
    "  final.tweets <- included.tweets[ ! included.tweets %in% excluded.tweets ]\n",
    "  \n",
    "  if( length( final.tweets) != 0 ){\n",
    "    tweets.df <- tweets.df[ c( final.tweets ) , ]\n",
    "  }\n",
    "\n",
    "  return( tweets.df)\n",
    "  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous set of functions are applied to each of the 8 birth control categories. Additionally, we add some steps to:\n",
    "- remove duplicated tweets. \n",
    "- remove male contractive tweets based on three expressions: \"male contraception\", \"male contraceptive\"and \"male birth control\"\n",
    "- remove emergency contraceptive tweets based on three expressions: \"emergency contraception\", \"emergency contraceptive\"and \"emergency birth control\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Reading the file Copper-T.csv\"\n",
      "[1] \"For the keyword Copper-T.csv there are a total of 337\"\n",
      "[1] \"Reading the file CopperIntrauterineDevice.csv\"\n",
      "[1] \"For the keyword CopperIntrauterineDevice.csv there are a total of 249\"\n",
      "[1] \"Reading the file CopperIntrauterineSystem.csv\"\n",
      "[1] \"For the keyword CopperIntrauterineSystem.csv there are a total of 0\"\n",
      "[1] \"Reading the file CopperIUD.csv\"\n",
      "[1] \"For the keyword CopperIUD.csv there are a total of 12083\"\n",
      "[1] \"Reading the file CopperIUS.csv\"\n",
      "[1] \"For the keyword CopperIUS.csv there are a total of 2\"\n",
      "[1] \"Reading the file CopperTBirthControl.csv\"\n",
      "[1] \"For the keyword CopperTBirthControl.csv there are a total of 6\"\n",
      "[1] \"Reading the file CopperTContraception.csv\"\n",
      "[1] \"For the keyword CopperTContraception.csv there are a total of 0\"\n",
      "[1] \"Reading the file CopperTContraceptive.csv\"\n",
      "[1] \"For the keyword CopperTContraceptive.csv there are a total of 3\"\n",
      "[1] \"Reading the file Cu-IUD.csv\"\n",
      "[1] \"For the keyword Cu-IUD.csv there are a total of 39\"\n",
      "[1] \"Reading the file Paragard.csv\"\n",
      "[1] \"For the keyword Paragard.csv there are a total of 9163\"\n",
      "[1] \"Reading the file Paraguard.csv\"\n",
      "[1] \"For the keyword Paraguard.csv there are a total of 5630\"\n",
      "[1] \"In total we have 27512 initial tweets for the category Copper_IUD/\"\n",
      "[1] \"Without duplicates, in total we have 25800 initial tweets for the category Copper_IUD/\"\n",
      "[1] \"There were 5 tweets about male birth control\"\n",
      "[1] \"Without male contraception, in total we have 25795 tweets for the category Copper_IUD/\"\n",
      "[1] \"There were 426 tweets about emergency contraception\"\n",
      "[1] \"Without emergency contraception, in total we have 25373 tweets for the category Copper_IUD/\"\n",
      "[1] \"Length of the tweets before cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "      8      82     122     129     148     605 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Length of the tweets after cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    1.0    68.0   107.0   117.2   139.0   288.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In total we have 24923 tweets that do not contain the keyword as username\"\n",
      "[1] \"In total we have 24122 tweets without duplicates\"\n",
      "[1] \"In total we have 17577 tweets that refer to a unique category\"\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Copper IUD #\n",
    "##############\n",
    "\n",
    "#1st. Unify the tweets\n",
    "tweets <-  unifyFiles( folder_path = \"/home/ec2-user/SageMaker/Tweets/\", \n",
    "                       folder_name = \"Copper_IUD/\")\n",
    "\n",
    "tweets <- tweets[ tweets$text != \"\", ]\n",
    "\n",
    "folder_name = \"Copper_IUD/\"\n",
    "print( paste0( \"In total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets <- tweets[!duplicated(tweets$text),]\n",
    "\n",
    "folder_name = \"Copper_IUD/\"\n",
    "print( paste0( \"Without duplicates, in total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "copperNoClean <- tweets\n",
    "\n",
    "#2nd. remove male BC, then count tweets\n",
    "maleContraception <- grep(\" male contraception\", tolower(tweets$text))\n",
    "maleContraceptive <- grep(\" male contraceptive\", tolower(tweets$text))\n",
    "maleBirthControl <- grep(\" male birth control\", tolower(tweets$text))\n",
    "\n",
    "maleBCrows <- c(maleContraception, maleContraceptive, maleBirthControl)\n",
    "print(paste0(\"There were \", length(maleBCrows), \" tweets about male birth control\"))\n",
    "\n",
    "tweets <- tweets[-maleBCrows, ]\n",
    "\n",
    "folder_name = \"Copper_IUD/\"\n",
    "print( paste0( \"Without male contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#3rd. remove emergency contraception, then count tweets\n",
    "emergencyContraception <- grep(\" emergency contraception\", tolower(tweets$text))\n",
    "emergencyContraceptive <- grep(\" emergency contraceptive\", tolower(tweets$text))\n",
    "emergencyBirthControl <- grep(\" emergency birth control\", tolower(tweets$text))\n",
    "\n",
    "emergencyContraceptionrows <- c(emergencyContraception, emergencyContraceptive, emergencyBirthControl)\n",
    "print(paste0(\"There were \", length(emergencyContraceptionrows), \" tweets about emergency contraception\"))\n",
    "\n",
    "tweets <- tweets[-emergencyContraceptionrows, ]\n",
    "\n",
    "folder_name = \"Copper_IUD/\"\n",
    "print( paste0( \"Without emergency contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#4th. Clean tweet text\n",
    "print(\"Length of the tweets before cleaning\")\n",
    "summary( nchar( tweets$text ) )\n",
    "ourCleanTweets <- cleanTweets( tweets.df = tweets )\n",
    "\n",
    "print(\"Length of the tweets after cleaning\")\n",
    "summary( nchar( ourCleanTweets$text ) )\n",
    "\n",
    "#5th. Clean by username\n",
    "ourCleanTweets <- usernameClean( tweets.df = ourCleanTweets, category = \"Copper IUD\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that do not contain the keyword as username\" ))\n",
    "\n",
    "#6th. Remove duplicates\n",
    "ourCleanTweets <- ourCleanTweets[!duplicated(ourCleanTweets$text),]\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets without duplicates\" ))\n",
    "\n",
    "#7th. Save the file to make Figures\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/SemiCleanAndAggregateTweets/copperIUD_SemiCleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "copperIUDSemiClean <- ourCleanTweets\n",
    "\n",
    "#8th. Remove tweets mentioning multiple categories\n",
    "ourCleanTweets <- tweetsUniqueCategory( tweets.df = ourCleanTweets, category = \"Copper IUD\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that refer to a unique category\" ))\n",
    "\n",
    "#9th. Save the file to proceed to the AWS comprehend analysis\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/CleanAndAggregateTweets/copperIUD_CleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "copperIUD <- ourCleanTweets\n",
    "rm(ourCleanTweets)\n",
    "rm(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Reading the file HormonalIntrauterineDevice.csv\"\n",
      "[1] \"For the keyword HormonalIntrauterineDevice.csv there are a total of 49\"\n",
      "[1] \"Reading the file HormonalIntrauterineSystem.csv\"\n",
      "[1] \"For the keyword HormonalIntrauterineSystem.csv there are a total of 8\"\n",
      "[1] \"Reading the file HormonalIUD.csv\"\n",
      "[1] \"For the keyword HormonalIUD.csv there are a total of 3298\"\n",
      "[1] \"Reading the file HormonalIUS.csv\"\n",
      "[1] \"For the keyword HormonalIUS.csv there are a total of 35\"\n",
      "[1] \"Reading the file HormoneIntrauterineDevice.csv\"\n",
      "[1] \"For the keyword HormoneIntrauterineDevice.csv there are a total of 2\"\n",
      "[1] \"Reading the file HormoneIntrauterineSystem.csv\"\n",
      "[1] \"For the keyword HormoneIntrauterineSystem.csv there are a total of 0\"\n",
      "[1] \"Reading the file HormoneIUD.csv\"\n",
      "[1] \"For the keyword HormoneIUD.csv there are a total of 219\"\n",
      "[1] \"Reading the file HormoneIUS.csv\"\n",
      "[1] \"For the keyword HormoneIUS.csv there are a total of 4\"\n",
      "[1] \"Reading the file KyleenaIntrauterineDevice.csv\"\n",
      "[1] \"For the keyword KyleenaIntrauterineDevice.csv there are a total of 0\"\n",
      "[1] \"Reading the file KyleenaIntrauterineSystem.csv\"\n",
      "[1] \"For the keyword KyleenaIntrauterineSystem.csv there are a total of 0\"\n",
      "[1] \"Reading the file KyleenaIUD.csv\"\n",
      "[1] \"For the keyword KyleenaIUD.csv there are a total of 457\"\n",
      "[1] \"Reading the file KyleenaIUS.csv\"\n",
      "[1] \"For the keyword KyleenaIUS.csv there are a total of 1\"\n",
      "[1] \"Reading the file LevonorgestrelIntrauterineDevice.csv\"\n",
      "[1] \"For the keyword LevonorgestrelIntrauterineDevice.csv there are a total of 62\"\n",
      "[1] \"Reading the file LevonorgestrelIntrauterineSystem.csv\"\n",
      "[1] \"For the keyword LevonorgestrelIntrauterineSystem.csv there are a total of 160\"\n",
      "[1] \"Reading the file LevonorgestrelIUD.csv\"\n",
      "[1] \"For the keyword LevonorgestrelIUD.csv there are a total of 156\"\n",
      "[1] \"Reading the file LevonorgestrelIUS.csv\"\n",
      "[1] \"For the keyword LevonorgestrelIUS.csv there are a total of 17\"\n",
      "[1] \"Reading the file LilettaIntrauterineDevice.csv\"\n",
      "[1] \"For the keyword LilettaIntrauterineDevice.csv there are a total of 5\"\n",
      "[1] \"Reading the file LilettaIntrauterineSystem.csv\"\n",
      "[1] \"For the keyword LilettaIntrauterineSystem.csv there are a total of 3\"\n",
      "[1] \"Reading the file LilettaIUD.csv\"\n",
      "[1] \"For the keyword LilettaIUD.csv there are a total of 284\"\n",
      "[1] \"Reading the file LilettaIUS.csv\"\n",
      "[1] \"For the keyword LilettaIUS.csv there are a total of 0\"\n",
      "[1] \"Reading the file LNG-IUD.csv\"\n",
      "[1] \"For the keyword LNG-IUD.csv there are a total of 36\"\n",
      "[1] \"Reading the file LNG-IUS.csv\"\n",
      "[1] \"For the keyword LNG-IUS.csv there are a total of 233\"\n",
      "[1] \"Reading the file LNGIUD.csv\"\n",
      "[1] \"For the keyword LNGIUD.csv there are a total of 42\"\n",
      "[1] \"Reading the file LNGIUS.csv\"\n",
      "[1] \"For the keyword LNGIUS.csv there are a total of 78\"\n",
      "[1] \"Reading the file MirenaIntrauterineDevice.csv\"\n",
      "[1] \"For the keyword MirenaIntrauterineDevice.csv there are a total of 25\"\n",
      "[1] \"Reading the file MirenaIntrauterineSystem.csv\"\n",
      "[1] \"For the keyword MirenaIntrauterineSystem.csv there are a total of 11\"\n",
      "[1] \"Reading the file MirenaIUD.csv\"\n",
      "[1] \"For the keyword MirenaIUD.csv there are a total of 12004\"\n",
      "[1] \"Reading the file MirenaIUS.csv\"\n",
      "[1] \"For the keyword MirenaIUS.csv there are a total of 218\"\n",
      "[1] \"Reading the file ProgesteroneIUD.csv\"\n",
      "[1] \"For the keyword ProgesteroneIUD.csv there are a total of 90\"\n",
      "[1] \"Reading the file ProgesteroneIUS.csv\"\n",
      "[1] \"For the keyword ProgesteroneIUS.csv there are a total of 7\"\n",
      "[1] \"Reading the file ProgestinIUD.csv\"\n",
      "[1] \"For the keyword ProgestinIUD.csv there are a total of 101\"\n",
      "[1] \"Reading the file SkylaIntrauterineDevice.csv\"\n",
      "[1] \"For the keyword SkylaIntrauterineDevice.csv there are a total of 2\"\n",
      "[1] \"Reading the file SkylaIntrauterineSystem.csv\"\n",
      "[1] \"For the keyword SkylaIntrauterineSystem.csv there are a total of 5\"\n",
      "[1] \"Reading the file SkylaIUD.csv\"\n",
      "[1] \"For the keyword SkylaIUD.csv there are a total of 784\"\n",
      "[1] \"Reading the file SkylaIUS.csv\"\n",
      "[1] \"For the keyword SkylaIUS.csv there are a total of 0\"\n",
      "[1] \"In total we have 18396 initial tweets for the category LNG-IUD/\"\n",
      "[1] \"Without duplicates, in total we have 16935 initial tweets for the category LNG-IUD/\"\n",
      "[1] \"There were 3 tweets about male birth control\"\n",
      "[1] \"Without male contraception, in total we have 16932 tweets for the category LNG-IUD/\"\n",
      "[1] \"There were 9 tweets about emergency contraception\"\n",
      "[1] \"Without emergency contraception, in total we have 16923 tweets for the category LNG-IUD/\"\n",
      "[1] \"Length of the tweets before cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "   19.0    93.0   130.0   135.9   150.0   448.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Length of the tweets after cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    7.0    71.0   108.0   117.4   135.0   286.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In total we have 16751 tweets that do not contain the keyword as username\"\n",
      "[1] \"In total we have 15520 tweets without duplicates\"\n",
      "[1] \"In total we have 11500 tweets that refer to a unique category\"\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# LNG IUD #\n",
    "###########\n",
    "\n",
    "#1st. Unify the tweets\n",
    "tweets <-  unifyFiles( folder_path = \"/home/ec2-user/SageMaker/Tweets/\", \n",
    "                       folder_name = \"LNG-IUD/\")\n",
    "\n",
    "tweets <- tweets[ tweets$text != \"\", ]\n",
    "\n",
    "folder_name = \"LNG-IUD/\"\n",
    "print( paste0( \"In total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets <- tweets[!duplicated(tweets$text),]\n",
    "\n",
    "folder_name = \"LNG-IUD/\"\n",
    "print( paste0( \"Without duplicates, in total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "lngNoClean <- tweets \n",
    "\n",
    "#2nd. remove male BC, then count tweets\n",
    "maleContraception <- grep(\" male contraception\", tolower(tweets$text))\n",
    "maleContraceptive <- grep(\" male contraceptive\", tolower(tweets$text))\n",
    "maleBirthControl <- grep(\" male birth control\", tolower(tweets$text))\n",
    "\n",
    "maleBCrows <- c(maleContraception, maleContraceptive, maleBirthControl)\n",
    "print(paste0(\"There were \", length(maleBCrows), \" tweets about male birth control\"))\n",
    "\n",
    "tweets <- tweets[-maleBCrows, ]\n",
    "\n",
    "folder_name = \"LNG-IUD/\"\n",
    "print( paste0( \"Without male contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#3rd. remove emergency contraception, then count tweets\n",
    "emergencyContraception <- grep(\" emergency contraception\", tolower(tweets$text))\n",
    "emergencyContraceptive <- grep(\" emergency contraceptive\", tolower(tweets$text))\n",
    "emergencyBirthControl <- grep(\" emergency birth control\", tolower(tweets$text))\n",
    "\n",
    "emergencyContraceptionrows <- c(emergencyContraception, emergencyContraceptive, emergencyBirthControl)\n",
    "print(paste0(\"There were \", length(emergencyContraceptionrows), \" tweets about emergency contraception\"))\n",
    "\n",
    "tweets <- tweets[-emergencyContraceptionrows, ]\n",
    "\n",
    "folder_name = \"LNG-IUD/\"\n",
    "print( paste0( \"Without emergency contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#4th. Clean tweet text\n",
    "print(\"Length of the tweets before cleaning\")\n",
    "summary( nchar( tweets$text ) )\n",
    "ourCleanTweets <- cleanTweets( tweets.df = tweets )\n",
    "\n",
    "print(\"Length of the tweets after cleaning\")\n",
    "summary( nchar( ourCleanTweets$text ) )\n",
    "\n",
    "#5th. Clean by username\n",
    "ourCleanTweets <- usernameClean( tweets.df = ourCleanTweets, category = \"LNG IUD\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that do not contain the keyword as username\" ))\n",
    "\n",
    "#6th. Remove duplicates\n",
    "ourCleanTweets <- ourCleanTweets[!duplicated(ourCleanTweets$text),]\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets without duplicates\" ))\n",
    "\n",
    "#7th. Save the file to make Figures\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/SemiCleanAndAggregateTweets/LNG-IUD_SemiCleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "LNG_IUDSemiClean <- ourCleanTweets\n",
    "\n",
    "#8th. Remove tweets mentioning multiple categories\n",
    "ourCleanTweets <- tweetsUniqueCategory( tweets.df = ourCleanTweets, category = \"LNG IUD\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that refer to a unique category\" ))\n",
    "\n",
    "#9th. Save the file to proceed to the AWS comprehend analysis\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/CleanAndAggregateTweets/LNG-IUD_CleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "LNG_IUD <- ourCleanTweets\n",
    "rm(ourCleanTweets)\n",
    "rm(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Reading the file IntrauterineDevice2006.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2006.csv there are a total of 0\"\n",
      "[1] \"Reading the file IntrauterineDevice2007.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2007.csv there are a total of 0\"\n",
      "[1] \"Reading the file IntrauterineDevice2008.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2008.csv there are a total of 2\"\n",
      "[1] \"Reading the file IntrauterineDevice2009.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2009.csv there are a total of 39\"\n",
      "[1] \"Reading the file IntrauterineDevice2010.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2010.csv there are a total of 296\"\n",
      "[1] \"Reading the file IntrauterineDevice2011.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2011.csv there are a total of 438\"\n",
      "[1] \"Reading the file IntrauterineDevice2012.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2012.csv there are a total of 758\"\n",
      "[1] \"Reading the file IntrauterineDevice2013.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2013.csv there are a total of 488\"\n",
      "[1] \"Reading the file IntrauterineDevice2014.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2014.csv there are a total of 529\"\n",
      "[1] \"Reading the file IntrauterineDevice2015.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2015.csv there are a total of 604\"\n",
      "[1] \"Reading the file IntrauterineDevice2016.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2016.csv there are a total of 340\"\n",
      "[1] \"Reading the file IntrauterineDevice2017.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2017.csv there are a total of 391\"\n",
      "[1] \"Reading the file IntrauterineDevice2018.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2018.csv there are a total of 485\"\n",
      "[1] \"Reading the file IntrauterineDevice2019.csv\"\n",
      "[1] \"For the keyword IntrauterineDevice2019.csv there are a total of 631\"\n",
      "[1] \"Reading the file IntrauterineSystem.csv\"\n",
      "[1] \"For the keyword IntrauterineSystem.csv there are a total of 819\"\n",
      "[1] \"Reading the file IUD2006.csv\"\n",
      "[1] \"For the keyword IUD2006.csv there are a total of 0\"\n",
      "[1] \"Reading the file IUD2007.csv\"\n",
      "[1] \"For the keyword IUD2007.csv there are a total of 16\"\n",
      "[1] \"Reading the file IUD2008.csv\"\n",
      "[1] \"For the keyword IUD2008.csv there are a total of 342\"\n",
      "[1] \"Reading the file IUD2009.csv\"\n",
      "[1] \"For the keyword IUD2009.csv there are a total of 643\"\n",
      "[1] \"Reading the file IUD2010.csv\"\n",
      "[1] \"For the keyword IUD2010.csv there are a total of 10828\"\n",
      "[1] \"Reading the file IUD2011.csv\"\n",
      "[1] \"For the keyword IUD2011.csv there are a total of 23572\"\n",
      "[1] \"Reading the file IUD2012.csv\"\n",
      "[1] \"For the keyword IUD2012.csv there are a total of 36846\"\n",
      "[1] \"Reading the file IUD2013.csv\"\n",
      "[1] \"For the keyword IUD2013.csv there are a total of 31590\"\n",
      "[1] \"Reading the file IUD2014.csv\"\n",
      "[1] \"For the keyword IUD2014.csv there are a total of 40816\"\n",
      "[1] \"Reading the file IUD2015.csv\"\n",
      "[1] \"For the keyword IUD2015.csv there are a total of 47268\"\n",
      "[1] \"Reading the file IUD2016.csv\"\n",
      "[1] \"For the keyword IUD2016.csv there are a total of 92870\"\n",
      "[1] \"Reading the file IUD2017.csv\"\n",
      "[1] \"For the keyword IUD2017.csv there are a total of 32892\"\n",
      "[1] \"Reading the file IUD2018.csv\"\n",
      "[1] \"For the keyword IUD2018.csv there are a total of 76198\"\n",
      "[1] \"Reading the file IUD2019_JanuarytoMay.csv\"\n",
      "[1] \"For the keyword IUD2019_JanuarytoMay.csv there are a total of 53157\"\n",
      "[1] \"Reading the file IUD2019_JunetoAugust.csv\"\n",
      "[1] \"For the keyword IUD2019_JunetoAugust.csv there are a total of 30915\"\n",
      "[1] \"Reading the file IUD2019_SeptoDecember.csv\"\n",
      "[1] \"For the keyword IUD2019_SeptoDecember.csv there are a total of 51328\"\n",
      "[1] \"Reading the file UterineJewelry.csv\"\n",
      "[1] \"For the keyword UterineJewelry.csv there are a total of 6\"\n",
      "[1] \"In total we have 535107 initial tweets for the category IUD/\"\n",
      "[1] \"Without duplicates, in total we have 483402 initial tweets for the category IUD/\"\n",
      "[1] \"There were 244 tweets about male birth control\"\n",
      "[1] \"Without male contraception, in total we have 483158 tweets for the category IUD/\"\n",
      "[1] \"There were 1884 tweets about emergency contraception\"\n",
      "[1] \"Without emergency contraception, in total we have 481290 tweets for the category IUD/\"\n",
      "[1] \"Length of the tweets before cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    3.0    80.0   124.0   150.2   183.0  1101.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Length of the tweets after cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    0.0    65.0    90.0   106.3   126.0   293.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In total we have 427515 tweets that do not contain the keyword as username\"\n",
      "[1] \"In total we have 397229 tweets without duplicates\"\n",
      "[1] \"In total we have 280037 tweets that refer to a unique category\"\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# IUD #\n",
    "#######\n",
    "\n",
    "#1st. Unify the tweets\n",
    "tweets <-  unifyFiles( folder_path = \"/home/ec2-user/SageMaker/Tweets/\", \n",
    "                       folder_name = \"IUD/\")\n",
    "\n",
    "tweets <- tweets[ tweets$text != \"\", ]\n",
    "\n",
    "folder_name = \"IUD/\"\n",
    "print( paste0( \"In total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets <- tweets[!duplicated(tweets$text),]\n",
    "\n",
    "folder_name = \"IUD/\"\n",
    "print( paste0( \"Without duplicates, in total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "IUD_NoClean <- tweets\n",
    "\n",
    "#2nd. remove male BC, then count tweets\n",
    "maleContraception <- grep(\" male contraception\", tolower(tweets$text))\n",
    "maleContraceptive <- grep(\" male contraceptive\", tolower(tweets$text))\n",
    "maleBirthControl <- grep(\" male birth control\", tolower(tweets$text))\n",
    "\n",
    "maleBCrows <- c(maleContraception, maleContraceptive, maleBirthControl)\n",
    "print(paste0(\"There were \", length(maleBCrows), \" tweets about male birth control\"))\n",
    "\n",
    "tweets <- tweets[-maleBCrows, ]\n",
    "\n",
    "folder_name = \"IUD/\"\n",
    "print( paste0( \"Without male contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#3rd. remove emergency contraception, then count tweets\n",
    "emergencyContraception <- grep(\" emergency contraception\", tolower(tweets$text))\n",
    "emergencyContraceptive <- grep(\" emergency contraceptive\", tolower(tweets$text))\n",
    "emergencyBirthControl <- grep(\" emergency birth control\", tolower(tweets$text))\n",
    "\n",
    "emergencyContraceptionrows <- c(emergencyContraception, emergencyContraceptive, emergencyBirthControl)\n",
    "print(paste0(\"There were \", length(emergencyContraceptionrows), \" tweets about emergency contraception\"))\n",
    "\n",
    "tweets <- tweets[-emergencyContraceptionrows, ]\n",
    "\n",
    "folder_name = \"IUD/\"\n",
    "print( paste0( \"Without emergency contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#4th. Clean tweet text\n",
    "print(\"Length of the tweets before cleaning\")\n",
    "summary( nchar( tweets$text ) )\n",
    "ourCleanTweets <- cleanTweets( tweets.df = tweets )\n",
    "\n",
    "print(\"Length of the tweets after cleaning\")\n",
    "summary( nchar( ourCleanTweets$text ) )\n",
    "\n",
    "#5th. Clean by username\n",
    "ourCleanTweets <- usernameClean( tweets.df = ourCleanTweets, category = \"IUD\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that do not contain the keyword as username\" ))\n",
    "\n",
    "#6th. Remove duplicates\n",
    "ourCleanTweets <- ourCleanTweets[!duplicated(ourCleanTweets$text),]\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets without duplicates\" ))\n",
    "\n",
    "#7th. Save the file to make Figures\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/SemiCleanAndAggregateTweets/IUD_SemiCleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "IUDSemiClean <- ourCleanTweets\n",
    "\n",
    "#8th. Remove tweets mentioning multiple categories\n",
    "ourCleanTweets <- tweetsUniqueCategory( tweets.df = ourCleanTweets, category = \"IUD\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that refer to a unique category\" ))\n",
    "\n",
    "#9th. Save the file to proceed to the AWS comprehend analysis\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/CleanAndAggregateTweets/IUD_CleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "IUD <- ourCleanTweets\n",
    "rm(ourCleanTweets)\n",
    "rm(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Reading the file ArmImplant.csv\"\n",
      "[1] \"For the keyword ArmImplant.csv there are a total of 8591\"\n",
      "[1] \"Reading the file B_C_Implant.csv\"\n",
      "[1] \"For the keyword B_C_Implant.csv there are a total of 43\"\n",
      "[1] \"Reading the file BCImplant.csv\"\n",
      "[1] \"For the keyword BCImplant.csv there are a total of 1952\"\n",
      "[1] \"Reading the file BirthControlImplant.csv\"\n",
      "[1] \"For the keyword BirthControlImplant.csv there are a total of 13742\"\n",
      "[1] \"Reading the file BirthControlRod.csv\"\n",
      "[1] \"For the keyword BirthControlRod.csv there are a total of 590\"\n",
      "[1] \"Reading the file ContraceptionImplant.csv\"\n",
      "[1] \"For the keyword ContraceptionImplant.csv there are a total of 366\"\n",
      "[1] \"Reading the file ContraceptionRod.csv\"\n",
      "[1] \"For the keyword ContraceptionRod.csv there are a total of 19\"\n",
      "[1] \"Reading the file ContraceptiveImplant.csv\"\n",
      "[1] \"For the keyword ContraceptiveImplant.csv there are a total of 11731\"\n",
      "[1] \"Reading the file ContraceptiveRod.csv\"\n",
      "[1] \"For the keyword ContraceptiveRod.csv there are a total of 89\"\n",
      "[1] \"Reading the file Implanon.csv\"\n",
      "[1] \"For the keyword Implanon.csv there are a total of 31787\"\n",
      "[1] \"Reading the file Nexplanon.csv\"\n",
      "[1] \"For the keyword Nexplanon.csv there are a total of 44249\"\n",
      "[1] \"In total we have 113159 initial tweets for the category Implant/\"\n",
      "[1] \"Without duplicates, in total we have 100552 initial tweets for the category Implant/\"\n",
      "[1] \"There were 38 tweets about male birth control\"\n",
      "[1] \"Without male contraception, in total we have 100514 tweets for the category Implant/\"\n",
      "[1] \"There were 16 tweets about emergency contraception\"\n",
      "[1] \"Without emergency contraception, in total we have 100498 tweets for the category Implant/\"\n",
      "[1] \"Length of the tweets before cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    8.0    68.0   105.0   112.8   137.0  2955.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Length of the tweets after cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    0.0    59.0    91.0   101.5   124.0   287.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In total we have 100495 tweets that do not contain the keyword as username\"\n",
      "[1] \"In total we have 94115 tweets without duplicates\"\n",
      "[1] \"In total we have 76356 tweets that refer to a unique category\"\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Implant #\n",
    "###########\n",
    "\n",
    "#1st. Unify the tweets\n",
    "tweets <-  unifyFiles( folder_path = \"/home/ec2-user/SageMaker/Tweets/\", \n",
    "                       folder_name = \"Implant/\")\n",
    "\n",
    "tweets <- tweets[ tweets$text != \"\", ]\n",
    "\n",
    "folder_name = \"Implant/\"\n",
    "print( paste0( \"In total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets <- tweets[!duplicated(tweets$text),]\n",
    "\n",
    "folder_name = \"Implant/\"\n",
    "print( paste0( \"Without duplicates, in total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "implantNoClean <- tweets \n",
    "\n",
    "#2nd. remove male BC, then count tweets\n",
    "maleContraception <- grep(\" male contraception\", tolower(tweets$text))\n",
    "maleContraceptive <- grep(\" male contraceptive\", tolower(tweets$text))\n",
    "maleBirthControl <- grep(\" male birth control\", tolower(tweets$text))\n",
    "\n",
    "maleBCrows <- c(maleContraception, maleContraceptive, maleBirthControl)\n",
    "print(paste0(\"There were \", length(maleBCrows), \" tweets about male birth control\"))\n",
    "\n",
    "tweets <- tweets[-maleBCrows, ]\n",
    "\n",
    "folder_name = \"Implant/\"\n",
    "print( paste0( \"Without male contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#3rd. remove emergency contraception, then count tweets\n",
    "emergencyContraception <- grep(\" emergency contraception\", tolower(tweets$text))\n",
    "emergencyContraceptive <- grep(\" emergency contraceptive\", tolower(tweets$text))\n",
    "emergencyBirthControl <- grep(\" emergency birth control\", tolower(tweets$text))\n",
    "\n",
    "emergencyContraceptionrows <- c(emergencyContraception, emergencyContraceptive, emergencyBirthControl)\n",
    "print(paste0(\"There were \", length(emergencyContraceptionrows), \" tweets about emergency contraception\"))\n",
    "\n",
    "tweets <- tweets[-emergencyContraceptionrows, ]\n",
    "\n",
    "folder_name = \"Implant/\"\n",
    "print( paste0( \"Without emergency contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#4th. Clean tweet text\n",
    "print(\"Length of the tweets before cleaning\")\n",
    "summary( nchar( tweets$text ) )\n",
    "ourCleanTweets <- cleanTweets( tweets.df = tweets )\n",
    "\n",
    "print(\"Length of the tweets after cleaning\")\n",
    "summary( nchar( ourCleanTweets$text ) )\n",
    "\n",
    "#5th. Clean by username\n",
    "ourCleanTweets <- usernameClean( tweets.df = ourCleanTweets, category = \"Implant\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that do not contain the keyword as username\" ))\n",
    "\n",
    "#6th. Remove duplicates\n",
    "ourCleanTweets <- ourCleanTweets[!duplicated(ourCleanTweets$text),]\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets without duplicates\" ))\n",
    "\n",
    "#7th. Save the file to make Figures\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/SemiCleanAndAggregateTweets/Implant_SemiCleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "ImplantSemiClean <- ourCleanTweets\n",
    "\n",
    "#8th. Remove tweets mentioning multiple categories\n",
    "ourCleanTweets <- tweetsUniqueCategory( tweets.df = ourCleanTweets, category = \"Implant\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that refer to a unique category\" ))\n",
    "\n",
    "#9th. Save the file to proceed to the AWS comprehend analysis\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/CleanAndAggregateTweets/Implant_CleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "Implant <- ourCleanTweets\n",
    "rm(ourCleanTweets)\n",
    "rm(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Reading the file BirthControlInjection.csv\"\n",
      "[1] \"For the keyword BirthControlInjection.csv there are a total of 2032\"\n",
      "[1] \"Reading the file BirthControlShot.csv\"\n",
      "[1] \"For the keyword BirthControlShot.csv there are a total of 35010\"\n",
      "[1] \"Reading the file ContraceptionInjection.csv\"\n",
      "[1] \"For the keyword ContraceptionInjection.csv there are a total of 594\"\n",
      "[1] \"Reading the file ContraceptionShot.csv\"\n",
      "[1] \"For the keyword ContraceptionShot.csv there are a total of 74\"\n",
      "[1] \"Reading the file ContraceptiveInjection.csv\"\n",
      "[1] \"For the keyword ContraceptiveInjection.csv there are a total of 7113\"\n",
      "[1] \"Reading the file ContraceptiveShot.csv\"\n",
      "[1] \"For the keyword ContraceptiveShot.csv there are a total of 800\"\n",
      "[1] \"Reading the file Depo-Provera.csv\"\n",
      "[1] \"For the keyword Depo-Provera.csv there are a total of 21829\"\n",
      "[1] \"Reading the file DepoBC.csv\"\n",
      "[1] \"For the keyword DepoBC.csv there are a total of 525\"\n",
      "[1] \"Reading the file DepoBirthControl.csv\"\n",
      "[1] \"For the keyword DepoBirthControl.csv there are a total of 1160\"\n",
      "[1] \"Reading the file DepoContraception.csv\"\n",
      "[1] \"For the keyword DepoContraception.csv there are a total of 39\"\n",
      "[1] \"Reading the file DepoContraceptive.csv\"\n",
      "[1] \"For the keyword DepoContraceptive.csv there are a total of 89\"\n",
      "[1] \"Reading the file DepoInjection.csv\"\n",
      "[1] \"For the keyword DepoInjection.csv there are a total of 2559\"\n",
      "[1] \"Reading the file DepoProvera.csv\"\n",
      "[1] \"For the keyword DepoProvera.csv there are a total of 22270\"\n",
      "[1] \"Reading the file DepoShot.csv\"\n",
      "[1] \"For the keyword DepoShot.csv there are a total of 72423\"\n",
      "[1] \"Reading the file DepotBirthControl.csv\"\n",
      "[1] \"For the keyword DepotBirthControl.csv there are a total of 16\"\n",
      "[1] \"Reading the file DepotContraception.csv\"\n",
      "[1] \"For the keyword DepotContraception.csv there are a total of 31\"\n",
      "[1] \"Reading the file DepotContraceptive.csv\"\n",
      "[1] \"For the keyword DepotContraceptive.csv there are a total of 46\"\n",
      "[1] \"Reading the file DepotInjection.csv\"\n",
      "[1] \"For the keyword DepotInjection.csv there are a total of 988\"\n",
      "[1] \"Reading the file DepotProvera.csv\"\n",
      "[1] \"For the keyword DepotProvera.csv there are a total of 106\"\n",
      "[1] \"Reading the file DepotShot.csv\"\n",
      "[1] \"For the keyword DepotShot.csv there are a total of 1353\"\n",
      "[1] \"Reading the file DMPA.csv\"\n",
      "[1] \"For the keyword DMPA.csv there are a total of 5137\"\n",
      "[1] \"Reading the file MedroxyprogesteroneAcetate.csv\"\n",
      "[1] \"For the keyword MedroxyprogesteroneAcetate.csv there are a total of 883\"\n",
      "[1] \"In total we have 175077 initial tweets for the category TheShot/\"\n",
      "[1] \"Without duplicates, in total we have 144462 initial tweets for the category TheShot/\"\n",
      "[1] \"There were 2894 tweets about male birth control\"\n",
      "[1] \"Without male contraception, in total we have 141587 tweets for the category TheShot/\"\n",
      "[1] \"There were 25 tweets about emergency contraception\"\n",
      "[1] \"Without emergency contraception, in total we have 141562 tweets for the category TheShot/\"\n",
      "[1] \"Length of the tweets before cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    4.0    61.0    92.0   100.4   131.0   564.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Length of the tweets after cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "   0.00   54.00   80.00   89.16  114.00  316.00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In total we have 139039 tweets that do not contain the keyword as username\"\n",
      "[1] \"In total we have 129025 tweets without duplicates\"\n",
      "[1] \"In total we have 117907 tweets that refer to a unique category\"\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# TheShot #\n",
    "###########\n",
    "\n",
    "#1st. Unify the tweets\n",
    "tweets <-  unifyFiles( folder_path = \"/home/ec2-user/SageMaker/Tweets/\", \n",
    "                       folder_name = \"TheShot/\")\n",
    "\n",
    "tweets <- tweets[ tweets$text != \"\", ]\n",
    "\n",
    "folder_name = \"TheShot/\"\n",
    "print( paste0( \"In total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets <- tweets[!duplicated(tweets$text),]\n",
    "\n",
    "folder_name = \"TheShot/\"\n",
    "print( paste0( \"Without duplicates, in total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "shotNoClean <- tweets\n",
    "\n",
    "#2nd. remove male BC, then count tweets\n",
    "maleContraception <- grep(\" male contraception\", tolower(tweets$text))\n",
    "maleContraceptive <- grep(\" male contraceptive\", tolower(tweets$text))\n",
    "maleBirthControl <- grep(\" male birth control\", tolower(tweets$text))\n",
    "\n",
    "maleBCrows <- c(maleContraception, maleContraceptive, maleBirthControl)\n",
    "print(paste0(\"There were \", length(maleBCrows), \" tweets about male birth control\"))\n",
    "\n",
    "tweets <- tweets[-maleBCrows, ]\n",
    "\n",
    "folder_name = \"TheShot/\"\n",
    "print( paste0( \"Without male contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#3rd. remove emergency contraception, then count tweets\n",
    "emergencyContraception <- grep(\" emergency contraception\", tolower(tweets$text))\n",
    "emergencyContraceptive <- grep(\" emergency contraceptive\", tolower(tweets$text))\n",
    "emergencyBirthControl <- grep(\" emergency birth control\", tolower(tweets$text))\n",
    "\n",
    "emergencyContraceptionrows <- c(emergencyContraception, emergencyContraceptive, emergencyBirthControl)\n",
    "print(paste0(\"There were \", length(emergencyContraceptionrows), \" tweets about emergency contraception\"))\n",
    "\n",
    "tweets <- tweets[-emergencyContraceptionrows, ]\n",
    "\n",
    "folder_name = \"TheShot/\"\n",
    "print( paste0( \"Without emergency contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#4th. Clean tweet text\n",
    "print(\"Length of the tweets before cleaning\")\n",
    "summary( nchar( tweets$text ) )\n",
    "ourCleanTweets <- cleanTweets( tweets.df = tweets )\n",
    "\n",
    "print(\"Length of the tweets after cleaning\")\n",
    "summary( nchar( ourCleanTweets$text ) )\n",
    "\n",
    "#5th. Clean by username\n",
    "ourCleanTweets <- usernameClean( tweets.df = ourCleanTweets, category = \"Shot\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that do not contain the keyword as username\" ))\n",
    "\n",
    "#6th. Remove duplicates\n",
    "ourCleanTweets <- ourCleanTweets[!duplicated(ourCleanTweets$text),]\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets without duplicates\" ))\n",
    "\n",
    "#7th. Save the file to make Figures\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/SemiCleanAndAggregateTweets/Shot_SemiCleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "shotSemiClean <- ourCleanTweets\n",
    "\n",
    "#8th. Remove tweets mentioning multiple categories\n",
    "ourCleanTweets <- tweetsUniqueCategory( tweets.df = ourCleanTweets, category = \"Shot\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that refer to a unique category\" ))\n",
    "\n",
    "#9th. Save the file to proceed to the AWS comprehend analysis\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/CleanAndAggregateTweets/Shot_CleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "shot <- ourCleanTweets\n",
    "rm(ourCleanTweets)\n",
    "rm(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Reading the file B_C_Pill.csv\"\n",
      "[1] \"For the keyword B_C_Pill.csv there are a total of 675\"\n",
      "[1] \"Reading the file BCPill.csv\"\n",
      "[1] \"For the keyword BCPill.csv there are a total of 17198\"\n",
      "[1] \"Reading the file BirthControlPill.csv\"\n",
      "[1] \"For the keyword BirthControlPill.csv there are a total of 32125\"\n",
      "[1] \"Reading the file CHCPill.csv\"\n",
      "[1] \"For the keyword CHCPill.csv there are a total of 2\"\n",
      "[1] \"Reading the file COCPill.csv\"\n",
      "[1] \"For the keyword COCPill.csv there are a total of 39\"\n",
      "[1] \"Reading the file CombinedB_C_P.csv\"\n",
      "[1] \"For the keyword CombinedB_C_P.csv there are a total of 1\"\n",
      "[1] \"Reading the file CombinedBCP.csv\"\n",
      "[1] \"For the keyword CombinedBCP.csv there are a total of 13\"\n",
      "[1] \"Reading the file CombinedHormonalContraceptive.csv\"\n",
      "[1] \"For the keyword CombinedHormonalContraceptive.csv there are a total of 98\"\n",
      "[1] \"Reading the file CombinedOCP.csv\"\n",
      "[1] \"For the keyword CombinedOCP.csv there are a total of 44\"\n",
      "[1] \"Reading the file ContraceptivePill.csv\"\n",
      "[1] \"For the keyword ContraceptivePill.csv there are a total of 63767\"\n",
      "[1] \"Reading the file O_C_Pill.csv\"\n",
      "[1] \"For the keyword O_C_Pill.csv there are a total of 4\"\n",
      "[1] \"Reading the file OCPill.csv\"\n",
      "[1] \"For the keyword OCPill.csv there are a total of 131\"\n",
      "[1] \"Reading the file OralBirthControl.csv\"\n",
      "[1] \"For the keyword OralBirthControl.csv there are a total of 2028\"\n",
      "[1] \"Reading the file OralContraception.csv\"\n",
      "[1] \"For the keyword OralContraception.csv there are a total of 8262\"\n",
      "[1] \"Reading the file OralContraceptive.csv\"\n",
      "[1] \"For the keyword OralContraceptive.csv there are a total of 25789\"\n",
      "[1] \"Reading the file ProgestinBCP.csv\"\n",
      "[1] \"For the keyword ProgestinBCP.csv there are a total of 0\"\n",
      "[1] \"Reading the file ProgestinOCP.csv\"\n",
      "[1] \"For the keyword ProgestinOCP.csv there are a total of 1\"\n",
      "[1] \"Reading the file ProgestinOnlyPill.csv\"\n",
      "[1] \"For the keyword ProgestinOnlyPill.csv there are a total of 314\"\n",
      "[1] \"In total we have 150491 initial tweets for the category ThePill/\"\n",
      "[1] \"Without duplicates, in total we have 125934 initial tweets for the category ThePill/\"\n",
      "[1] \"There were 9413 tweets about male birth control\"\n",
      "[1] \"Without male contraception, in total we have 116562 tweets for the category ThePill/\"\n",
      "[1] \"There were 2453 tweets about emergency contraception\"\n",
      "[1] \"Without emergency contraception, in total we have 114123 tweets for the category ThePill/\"\n",
      "[1] \"Length of the tweets before cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    7.0    90.0   124.0   130.5   140.0   697.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Length of the tweets after cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "      4      68      98     108     124     288 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In total we have 113887 tweets that do not contain the keyword as username\"\n",
      "[1] \"In total we have 97568 tweets without duplicates\"\n",
      "[1] \"In total we have 90836 tweets that refer to a unique category\"\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# The pill #\n",
    "############\n",
    "\n",
    "#1st. Unify the tweets\n",
    "tweets <-  unifyFiles( folder_path = \"/home/ec2-user/SageMaker/Tweets/\", \n",
    "                       folder_name = \"ThePill/\")\n",
    "\n",
    "tweets <- tweets[ tweets$text != \"\", ]\n",
    "\n",
    "folder_name = \"ThePill/\"\n",
    "print( paste0( \"In total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets <- tweets[!duplicated(tweets$text),]\n",
    "\n",
    "folder_name = \"ThePill/\"\n",
    "print( paste0( \"Without duplicates, in total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "pillNoClean <- tweets\n",
    "\n",
    "#2nd. remove male BC, then count tweets\n",
    "maleContraception <- grep(\" male contraception\", tolower(tweets$text))\n",
    "maleContraceptive <- grep(\" male contraceptive\", tolower(tweets$text))\n",
    "maleBirthControl <- grep(\" male birth control\", tolower(tweets$text))\n",
    "\n",
    "maleBCrows <- c(maleContraception, maleContraceptive, maleBirthControl)\n",
    "print(paste0(\"There were \", length(maleBCrows), \" tweets about male birth control\"))\n",
    "\n",
    "tweets <- tweets[-maleBCrows, ]\n",
    "\n",
    "folder_name = \"ThePill/\"\n",
    "print( paste0( \"Without male contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#3rd. remove emergency contraception, then count tweets\n",
    "emergencyContraception <- grep(\" emergency contraception\", tolower(tweets$text))\n",
    "emergencyContraceptive <- grep(\" emergency contraceptive\", tolower(tweets$text))\n",
    "emergencyBirthControl <- grep(\" emergency birth control\", tolower(tweets$text))\n",
    "\n",
    "emergencyContraceptionrows <- c(emergencyContraception, emergencyContraceptive, emergencyBirthControl)\n",
    "print(paste0(\"There were \", length(emergencyContraceptionrows), \" tweets about emergency contraception\"))\n",
    "\n",
    "tweets <- tweets[-emergencyContraceptionrows, ]\n",
    "\n",
    "folder_name = \"ThePill/\"\n",
    "print( paste0( \"Without emergency contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#4th. Clean tweet text\n",
    "print(\"Length of the tweets before cleaning\")\n",
    "summary( nchar( tweets$text ) )\n",
    "ourCleanTweets <- cleanTweets( tweets.df = tweets )\n",
    "\n",
    "print(\"Length of the tweets after cleaning\")\n",
    "summary( nchar( ourCleanTweets$text ) )\n",
    "\n",
    "#5th. Clean by username\n",
    "ourCleanTweets <- usernameClean( tweets.df = ourCleanTweets, category = \"Pill\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that do not contain the keyword as username\" ))\n",
    "\n",
    "#6th. Remove duplicates\n",
    "ourCleanTweets <- ourCleanTweets[!duplicated(ourCleanTweets$text),]\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets without duplicates\" ))\n",
    "\n",
    "#7th. Save the file to make Figures\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/SemiCleanAndAggregateTweets/Pill_SemiCleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "pillSemiClean <- ourCleanTweets\n",
    "\n",
    "#8th. Remove tweets mentioning multiple categories\n",
    "ourCleanTweets <- tweetsUniqueCategory( tweets.df = ourCleanTweets, category = \"Pill\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that refer to a unique category\" ))\n",
    "\n",
    "#9th. Save the file to proceed to the AWS comprehend analysis\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/CleanAndAggregateTweets/Pill_CleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "pill <- ourCleanTweets\n",
    "rm(ourCleanTweets)\n",
    "rm(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Reading the file Annovera.csv\"\n",
      "[1] \"For the keyword Annovera.csv there are a total of 314\"\n",
      "[1] \"Reading the file BirthControlRing.csv\"\n",
      "[1] \"For the keyword BirthControlRing.csv there are a total of 1313\"\n",
      "[1] \"Reading the file ContraceptionRing.csv\"\n",
      "[1] \"For the keyword ContraceptionRing.csv there are a total of 194\"\n",
      "[1] \"Reading the file ContraceptiveRing.csv\"\n",
      "[1] \"For the keyword ContraceptiveRing.csv there are a total of 1078\"\n",
      "[1] \"Reading the file Nuva_Ring.csv\"\n",
      "[1] \"For the keyword Nuva_Ring.csv there are a total of 37068\"\n",
      "[1] \"Reading the file Nuva-ring.csv\"\n",
      "[1] \"For the keyword Nuva-ring.csv there are a total of 29400\"\n",
      "[1] \"Reading the file Nuvaring.csv\"\n",
      "[1] \"For the keyword Nuvaring.csv there are a total of 22319\"\n",
      "[1] \"Reading the file VaginalRing.csv\"\n",
      "[1] \"For the keyword VaginalRing.csv there are a total of 15909\"\n",
      "[1] \"In total we have 107595 initial tweets for the category TheRing/\"\n",
      "[1] \"Without duplicates, in total we have 72886 initial tweets for the category TheRing/\"\n",
      "[1] \"There were 18 tweets about male birth control\"\n",
      "[1] \"Without male contraception, in total we have 72868 tweets for the category TheRing/\"\n",
      "[1] \"There were 16 tweets about emergency contraception\"\n",
      "[1] \"Without emergency contraception, in total we have 72852 tweets for the category TheRing/\"\n",
      "[1] \"Length of the tweets before cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    9.0    68.0   100.0   105.3   134.0   630.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Length of the tweets after cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "   4.00   54.00   78.00   86.71  110.00  288.00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In total we have 72261 tweets that do not contain the keyword as username\"\n",
      "[1] \"In total we have 64647 tweets without duplicates\"\n",
      "[1] \"In total we have 56283 tweets that refer to a unique category\"\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# The Ring #\n",
    "############\n",
    "\n",
    "#1st. Unify the tweets\n",
    "tweets <-  unifyFiles( folder_path = \"/home/ec2-user/SageMaker/Tweets/\", \n",
    "                       folder_name = \"TheRing/\")\n",
    "\n",
    "tweets <- tweets[ tweets$text != \"\", ]\n",
    "\n",
    "folder_name = \"TheRing/\"\n",
    "print( paste0( \"In total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets <- tweets[!duplicated(tweets$text),]\n",
    "\n",
    "folder_name = \"TheRing/\"\n",
    "print( paste0( \"Without duplicates, in total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "ringNoClean <- tweets\n",
    "\n",
    "#2nd. remove male BC, then count tweets\n",
    "maleContraception <- grep(\" male contraception\", tolower(tweets$text))\n",
    "maleContraceptive <- grep(\" male contraceptive\", tolower(tweets$text))\n",
    "maleBirthControl <- grep(\" male birth control\", tolower(tweets$text))\n",
    "\n",
    "maleBCrows <- c(maleContraception, maleContraceptive, maleBirthControl)\n",
    "print(paste0(\"There were \", length(maleBCrows), \" tweets about male birth control\"))\n",
    "\n",
    "tweets <- tweets[-maleBCrows, ]\n",
    "\n",
    "folder_name = \"TheRing/\"\n",
    "print( paste0( \"Without male contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#3rd. remove emergency contraception, then count tweets\n",
    "emergencyContraception <- grep(\" emergency contraception\", tolower(tweets$text))\n",
    "emergencyContraceptive <- grep(\" emergency contraceptive\", tolower(tweets$text))\n",
    "emergencyBirthControl <- grep(\" emergency birth control\", tolower(tweets$text))\n",
    "\n",
    "emergencyContraceptionrows <- c(emergencyContraception, emergencyContraceptive, emergencyBirthControl)\n",
    "print(paste0(\"There were \", length(emergencyContraceptionrows), \" tweets about emergency contraception\"))\n",
    "\n",
    "tweets <- tweets[-emergencyContraceptionrows, ]\n",
    "\n",
    "folder_name = \"TheRing/\"\n",
    "print( paste0( \"Without emergency contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#4th. Clean tweet text\n",
    "print(\"Length of the tweets before cleaning\")\n",
    "summary( nchar( tweets$text ) )\n",
    "ourCleanTweets <- cleanTweets( tweets.df = tweets )\n",
    "\n",
    "print(\"Length of the tweets after cleaning\")\n",
    "summary( nchar( ourCleanTweets$text ) )\n",
    "\n",
    "#5th. Clean by username\n",
    "ourCleanTweets <- usernameClean( tweets.df = ourCleanTweets, category = \"Ring\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that do not contain the keyword as username\" ))\n",
    "\n",
    "#6th. Remove duplicates\n",
    "ourCleanTweets <- ourCleanTweets[!duplicated(ourCleanTweets$text),]\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets without duplicates\" ))\n",
    "\n",
    "#7th. Save the file to make Figures\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/SemiCleanAndAggregateTweets/Ring_SemiCleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "ringSemiClean <- ourCleanTweets\n",
    "\n",
    "#8th. Remove tweets mentioning multiple categories\n",
    "ourCleanTweets <- tweetsUniqueCategory( tweets.df = ourCleanTweets, category = \"Ring\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that refer to a unique category\" ))\n",
    "\n",
    "#9th. Save the file to proceed to the AWS comprehend analysis\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/CleanAndAggregateTweets/Ring_CleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "ring <- ourCleanTweets\n",
    "rm(ourCleanTweets)\n",
    "rm(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Reading the file BCPatch.csv\"\n",
      "[1] \"For the keyword BCPatch.csv there are a total of 1957\"\n",
      "[1] \"Reading the file BirthControlPatch.csv\"\n",
      "[1] \"For the keyword BirthControlPatch.csv there are a total of 13362\"\n",
      "[1] \"Reading the file ContraceptionPatch.csv\"\n",
      "[1] \"For the keyword ContraceptionPatch.csv there are a total of 240\"\n",
      "[1] \"Reading the file ContraceptivePatch.csv\"\n",
      "[1] \"For the keyword ContraceptivePatch.csv there are a total of 3175\"\n",
      "[1] \"Reading the file OrthoEvra.csv\"\n",
      "[1] \"For the keyword OrthoEvra.csv there are a total of 3191\"\n",
      "[1] \"Reading the file Xulane.csv\"\n",
      "[1] \"For the keyword Xulane.csv there are a total of 610\"\n",
      "[1] \"In total we have 22535 initial tweets for the category ThePatch/\"\n",
      "[1] \"Without duplicates, in total we have 19656 initial tweets for the category ThePatch/\"\n",
      "[1] \"There were 73 tweets about male birth control\"\n",
      "[1] \"Without male contraception, in total we have 19583 tweets for the category ThePatch/\"\n",
      "[1] \"There were 4 tweets about emergency contraception\"\n",
      "[1] \"Without emergency contraception, in total we have 19579 tweets for the category ThePatch/\"\n",
      "[1] \"Length of the tweets before cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "    8.0    70.0    98.0   102.2   132.0   481.0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Length of the tweets after cleaning\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "   5.00   52.00   78.00   83.31  110.00  282.00 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"In total we have 19569 tweets that do not contain the keyword as username\"\n",
      "[1] \"In total we have 16513 tweets without duplicates\"\n",
      "[1] \"In total we have 14568 tweets that refer to a unique category\"\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# The Patch #\n",
    "############\n",
    "\n",
    "#1st. Unify the tweets\n",
    "tweets <-  unifyFiles( folder_path = \"/home/ec2-user/SageMaker/Tweets/\", \n",
    "                       folder_name = \"ThePatch/\")\n",
    "\n",
    "tweets <- tweets[ tweets$text != \"\", ]\n",
    "\n",
    "folder_name = \"ThePatch/\"\n",
    "print( paste0( \"In total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets <- tweets[!duplicated(tweets$text),]\n",
    "\n",
    "folder_name = \"ThePatch/\"\n",
    "print( paste0( \"Without duplicates, in total we have \", nrow( tweets ), \" initial tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "patchNoClean <- tweets\n",
    "\n",
    "#2nd. remove male BC, then count tweets\n",
    "maleContraception <- grep(\" male contraception\", tolower(tweets$text))\n",
    "maleContraceptive <- grep(\" male contraceptive\", tolower(tweets$text))\n",
    "maleBirthControl <- grep(\" male birth control\", tolower(tweets$text))\n",
    "\n",
    "maleBCrows <- c(maleContraception, maleContraceptive, maleBirthControl)\n",
    "print(paste0(\"There were \", length(maleBCrows), \" tweets about male birth control\"))\n",
    "\n",
    "tweets <- tweets[-maleBCrows, ]\n",
    "\n",
    "folder_name = \"ThePatch/\"\n",
    "print( paste0( \"Without male contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#3rd. remove emergency contraception, then count tweets\n",
    "emergencyContraception <- grep(\" emergency contraception\", tolower(tweets$text))\n",
    "emergencyContraceptive <- grep(\" emergency contraceptive\", tolower(tweets$text))\n",
    "emergencyBirthControl <- grep(\" emergency birth control\", tolower(tweets$text))\n",
    "\n",
    "emergencyContraceptionrows <- c(emergencyContraception, emergencyContraceptive, emergencyBirthControl)\n",
    "print(paste0(\"There were \", length(emergencyContraceptionrows), \" tweets about emergency contraception\"))\n",
    "\n",
    "tweets <- tweets[-emergencyContraceptionrows, ]\n",
    "\n",
    "folder_name = \"ThePatch/\"\n",
    "print( paste0( \"Without emergency contraception, in total we have \", nrow( tweets ), \" tweets for the category \", folder_name ))\n",
    "tweets$text <- as.character( tweets$text )\n",
    "\n",
    "#4th. Clean tweet text\n",
    "print(\"Length of the tweets before cleaning\")\n",
    "summary( nchar( tweets$text ) )\n",
    "ourCleanTweets <- cleanTweets( tweets.df = tweets )\n",
    "\n",
    "print(\"Length of the tweets after cleaning\")\n",
    "summary( nchar( ourCleanTweets$text ) )\n",
    "\n",
    "#5th. Clean by username\n",
    "ourCleanTweets <- usernameClean( tweets.df = ourCleanTweets, category = \"Patch\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that do not contain the keyword as username\" ))\n",
    "\n",
    "#6th. Remove duplicates\n",
    "ourCleanTweets <- ourCleanTweets[!duplicated(ourCleanTweets$text),]\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets without duplicates\" ))\n",
    "\n",
    "#7th. Save the file to make Figures\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/SemiCleanAndAggregateTweets/Patch_SemiCleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "patchSemiClean <- ourCleanTweets\n",
    "\n",
    "#8th. Remove tweets mentioning multiple categories\n",
    "ourCleanTweets <- tweetsUniqueCategory( tweets.df = ourCleanTweets, category = \"Patch\")\n",
    "print( paste0( \"In total we have \", nrow(ourCleanTweets), \" tweets that refer to a unique category\" ))\n",
    "\n",
    "#9th. Save the file to proceed to the AWS comprehend analysis\n",
    "write.table( ourCleanTweets, \n",
    "            file = \"/home/ec2-user/SageMaker/CleanAndAggregateTweets/Patch_CleanTweets.txt\", \n",
    "            col.names = TRUE, \n",
    "            row.names = FALSE, \n",
    "            sep = \"\\t\", \n",
    "            quote = FALSE )\n",
    "\n",
    "patch <- ourCleanTweets\n",
    "rm(ourCleanTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>989627</li>\n",
       "\t<li>12</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 989627\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 989627\n",
       "2. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 989627     12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####################################\n",
    "## Summary of the tweets extracted ##\n",
    "#####################################\n",
    "\n",
    "#all tweets extracted\n",
    "totalTweets <- rbind( copperNoClean, lngNoClean, IUD_NoClean,\n",
    "                     implantNoClean,shotNoClean,pillNoClean,\n",
    "                     ringNoClean, patchNoClean)\n",
    "\n",
    "\n",
    "\n",
    "dim(totalTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R version 3.6.1 (2019-07-05)\n",
       "Platform: x86_64-conda_cos6-linux-gnu (64-bit)\n",
       "Running under: Amazon Linux AMI 2018.03\n",
       "\n",
       "Matrix products: default\n",
       "BLAS/LAPACK: /home/ec2-user/anaconda3/envs/R/lib/R/lib/libRblas.so\n",
       "\n",
       "locale:\n",
       " [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n",
       " [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n",
       " [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n",
       " [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n",
       " [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n",
       "[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n",
       "\n",
       "attached base packages:\n",
       "[1] stats     graphics  grDevices utils     datasets  methods   base     \n",
       "\n",
       "other attached packages:\n",
       "[1] forcats_0.4.0   stringr_1.4.0   purrr_0.3.2     readr_1.3.1    \n",
       "[5] tidyr_0.8.3     tibble_2.1.1    ggplot2_3.1.1   tidyverse_1.2.1\n",
       "[9] dplyr_0.8.3    \n",
       "\n",
       "loaded via a namespace (and not attached):\n",
       " [1] Rcpp_1.0.1       cellranger_1.1.0 pillar_1.3.1     compiler_3.6.1  \n",
       " [5] plyr_1.8.4       base64enc_0.1-3  tools_3.6.1      digest_0.6.18   \n",
       " [9] uuid_0.1-2       lubridate_1.7.4  jsonlite_1.6     evaluate_0.13   \n",
       "[13] nlme_3.1-139     gtable_0.3.0     lattice_0.20-38  pkgconfig_2.0.2 \n",
       "[17] rlang_0.4.2      cli_1.1.0        rstudioapi_0.10  IRdisplay_0.7.0 \n",
       "[21] IRkernel_0.8.15  haven_2.1.0      withr_2.1.2      xml2_1.2.0      \n",
       "[25] httr_1.4.0       repr_0.19.2      hms_0.4.2        generics_0.0.2  \n",
       "[29] grid_3.6.1       tidyselect_0.2.5 glue_1.3.1       R6_2.4.0        \n",
       "[33] readxl_1.3.1     pbdZMQ_0.3-3     modelr_0.1.4     magrittr_1.5    \n",
       "[37] backports_1.1.4  scales_1.0.0     htmltools_0.3.6  rvest_0.3.3     \n",
       "[41] assertthat_0.2.1 colorspace_1.4-1 stringi_1.4.3    lazyeval_0.2.2  \n",
       "[45] munsell_0.5.0    broom_0.5.2      crayon_1.3.4    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sessionInfo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
